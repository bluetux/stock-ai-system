# version: '3.8'

services:
  nginx:
    image: nginx:latest
    container_name: nginx
    networks:
      - stock-network
    ports:
      - "80:80" # 단일 포트 사용
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend
      # - jupyter
      - comfy
    restart: always
  db:
    image: postgres
    container_name: db
    hostname: db
    build: ./postgres
    networks:
      - stock-network
    env_file:
      - ./postgres/.env
    user: "1001:1001"
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
      - ./postgres/entrypoint.sh:/docker-entrypoint-initdb.d/entrypoint.sh
    deploy:
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
      interval: 10s
      retries: 5
      timeout: 5s
    restart: always

  collector:
    image: data-collector
    container_name: collector
    hostname: collector
    build: ./data-collector
    networks:
      - stock-network
    volumes:
      - ./data/raw:/data
      - ./data-collector:/app
    depends_on:
      db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  quant:
    image: quant-analyzer
    container_name: quant
    hostname: quant
    build: ./quant-analyzer
    networks:
      - stock-network
    volumes:
      - ./data/processed:/data
      - ./quant-analyzer:/app
    depends_on:
      collector:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  ai:
    image: ai-predictor
    container_name: ai
    hostname: ai
    build:
      context: .
      dockerfile: ai-predictor/Dockerfile
    networks:
      - stock-network
    volumes:
      - ./data/models:/data
      - ./ai-predictor:/app
    depends_on:
      quant:
        condition: service_started
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
        limits:
          memory: 4G
          cpus: "1"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_HOME=/usr/local/cuda
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64
      - XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/nvidia-cuda-toolkit/libdevice
    runtime: nvidia
  frontend:
    image: frontend-web
    container_name: frontend
    hostname: frontend
    build: ./frontend
    networks:
      - stock-network
    ports:
      - "5173:5173"
    environment:
      - NODE_OPTIONS=--openssl-legacy-provider
      - CHOKIDAR_USEPOLLING=true
      - HOST=0.0.0.0
    volumes:
      - ./frontend:/app
      # - /app/node_modules
    stdin_open: true
    tty: true
    restart: always


  web-api:
    image: web-api-backend
    container_name: web-api
    hostname: web-api
    build: ./web-api
    networks:
      - stock-network
    ports:
      - "8000:8000"
    volumes:
      - ./web-api:/app
    # environment:
    #   - PYTHONUNBUFFERED=1
    env_file:
      - .env
    depends_on:
      db:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
    restart: always
  comfy:
    image: comfy-ui
    container_name: comfy
    hostname: comfy
    user: "1000:1000"
    build: ./comfy-ui
    networks:
      - stock-network
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - COMFYUI_LOAD_CUSTOM_NODES=1
    # ports:
    #   - 7860:7860
    deploy:
      resources:
        limits:
          memory: 20G
          cpus: "4"
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    runtime: nvidia
    volumes:
      - ./comfy-ui:/home/user/app
    restart: always

networks:
  stock-network:
    driver: bridge
